import datetime
import pandas as pd

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.postgres.operators.postgres import PostgresOperator


POSTGRES_CONN_ID = "data_eng"
CSV_FILE_INGESTION_2010_2019 = "/opt/airflow/dags/ingestion_zone/la_crime_2010_2019.csv"
CSV_FILE_INGESTION_2020_PRESENT = (
    "/opt/airflow/dags/ingestion_zone/la_crime_2020_present.csv"
)
CSV_COMBINED = "/opt/airflow/dags/ingestion_zone/la_crime_combined.csv"


def merge_datasets():
    df1 = pd.read_csv(CSV_FILE_INGESTION_2010_2019)
    df2 = pd.read_csv(CSV_FILE_INGESTION_2020_PRESENT)

    # first dataset has an extra space ' ' in the 'Area' column name
    df1.rename(columns={"AREA ": "AREA"}, inplace=True)

    df_combined = pd.concat([df1, df2], ignore_index=True)
    df_combined.to_csv(CSV_COMBINED, index=False)
    print(f"The files were successfully combined and saved: {CSV_COMBINED}")


def clean_dataset():
    df = pd.read_csv(CSV_COMBINED)

    columns_to_delete = [
        "Date Rptd",
        "TIME OCC",
        "AREA",
        "AREA NAME",
        "Rpt Dist No",
        "Mocodes",
        "Vict Age",
        "Vict Sex",
        "Vict Descent",
        "Crm Cd 1",
        "Crm Cd 2",
        "Crm Cd 3",
        "Crm Cd 4",
        "Cross Street",
        "Part 1-2",
        "LOCATION",
    ]

    df.drop(columns=columns_to_delete, inplace=True)

    df.rename(
        columns={
            "DR_NO": "record_number",
            "DATE OCC": "date",
            "Crm Cd": "crime_code",
            "Crm Cd Desc": "crime_committed_description",
            "Premis Cd": "type_of_structure_vehicle_location_code",
            "Premis Desc": "type_of_structure_vehicle_location_description",
            "Weapon Used Cd": "type_of_weapon_code",
            "Weapon Desc": "type_of_weapon_description",
            "Status": "status_of_the_crime_code",
            "Status Desc": "status_of_the_crime_description",
            "LAT": "latitude",
            "LON": "longitude",
        },
        inplace=True,
    )

    # fill empty values so type cast to int will work
    df["type_of_structure_vehicle_location_code"] = df[
        "type_of_structure_vehicle_location_code"
    ].fillna(0.0)
    df["type_of_weapon_code"] = df["type_of_weapon_code"].fillna(0.0)

    # fix types
    df["type_of_structure_vehicle_location_code"] = df[
        "type_of_structure_vehicle_location_code"
    ].astype(int)
    df["type_of_weapon_code"] = df["type_of_weapon_code"].astype(int)

    # cut "12:00:00 AM" out of string
    df["date"] = df["date"].str[:10].astype(str)

    # turn american month/day/year into year-month-day like other datasets
    df["date"] = df["date"].apply(
        lambda x: datetime.datetime.strptime(x, "%m/%d/%Y").strftime("%Y-%m-%d")
    )

    df.to_csv(CSV_COMBINED, index=False)
    print(f"The dataset was successfully cleaned and saved: {CSV_COMBINED}")


def get_airflow_sql_connection():
    pg_hook = PostgresHook.get_hook(POSTGRES_CONN_ID)
    connection = pg_hook.get_conn()
    return connection


def move_to_postgres():
    conn = get_airflow_sql_connection()
    cur = conn.cursor()
    conn.commit()

    with open(CSV_COMBINED) as f:
        cur.copy_expert(
            "COPY staging_crime_reports_la(record_number, date, crime_code, crime_committed_description, type_of_structure_vehicle_location_code, type_of_structure_vehicle_location_description, type_of_weapon_code, type_of_weapon_description, status_of_the_crime_code, status_of_the_crime_description, latitude, longitude) FROM stdin WITH (FORMAT csv, HEADER, DELIMITER ',');",
            f,
        )

    conn.commit()
    cur.close()
    conn.close()


default_args_dict = {
    "start_date": datetime.datetime.now(datetime.timezone.utc),
    "concurrency": 1,
    "retries": 1,
    "retry_delay": datetime.timedelta(minutes=5),
}

dag = DAG(
    dag_id="wrangle_crime_reports_los_angeles",
    default_args=default_args_dict,
    schedule="@weekly",
    catchup=False,
)

task_merge_datasets = PythonOperator(
    task_id="task_merge_datasets",
    python_callable=merge_datasets,
    dag=dag,
)

task_clean_dataset = PythonOperator(
    task_id="task_clean_dataset",
    python_callable=clean_dataset,
    dag=dag,
)

task_create_sql_table = PostgresOperator(
    task_id="task_create_sql_table",
    postgres_conn_id="data_eng",
    sql="CREATE TABLE IF NOT EXISTS staging_crime_reports_la (id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY, record_number BIGINT, date DATE, crime_code INTEGER, crime_committed_description TEXT, type_of_structure_vehicle_location_code INTEGER, type_of_structure_vehicle_location_description TEXT, type_of_weapon_code INTEGER, type_of_weapon_description TEXT, status_of_the_crime_code TEXT, status_of_the_crime_description TEXT, latitude NUMERIC, longitude NUMERIC);",
    dag=dag,
)

task_move_to_postgres = PythonOperator(
    task_id="task_move_to_postgres",
    python_callable=move_to_postgres,
    dag=dag,
)

(
    task_merge_datasets
    >> task_clean_dataset
    >> task_create_sql_table
    >> task_move_to_postgres
)
